# Story 3.1: AI Content Synthesis Engine

## Status
Done

## Story
**As the** system,
**I want** to process raw content from multiple sources into coherent, personalized insights using AI synthesis,
**so that** users receive connected intelligence rather than fragmented information pieces.

## Acceptance Criteria
1. OpenAI API integration with prompt engineering for professional content synthesis
2. Content processing pipeline that combines articles from multiple sources into unified insights
3. User context incorporation including professional background, current tool stack, and interest areas
4. Synthesis quality validation with confidence scoring and error handling for low-quality outputs
5. Source attribution preservation throughout synthesis process for transparency and credibility
6. Batch processing capability to handle daily content volumes efficiently within API rate limits
7. Content categorization and tagging system for organized insight delivery
8. Fallback mechanisms for API failures including content queuing and retry logic

## Tasks / Subtasks
- [x] Implement OpenAI API integration (AC: 1)
  - [x] Create `lib/openai.ts` with GPT-5 mini client using latest API conventions (reasoning_effort, verbosity parameters)
  - [x] Build prompt engineering system for content synthesis
  - [x] Add API key validation and environment variable handling
  - [x] Implement rate limiting for OpenAI API calls
- [x] Build content processing pipeline (AC: 2, 3)
  - [x] Create `lib/content-synthesis.ts` for main synthesis engine
  - [x] Implement multi-source content aggregation logic
  - [x] Build user context injection from UserPreferences model
  - [x] Add professional background and tool stack personalization
- [x] Implement synthesis quality validation (AC: 4)
  - [x] Create content quality scoring system
  - [x] Add confidence metrics for AI-generated insights
  - [x] Implement error handling for poor-quality outputs
  - [x] Build content validation and retry mechanisms
- [x] Build source attribution system (AC: 5)
  - [x] Extend synthesis to preserve original source metadata
  - [x] Implement transparent source linking in generated content
  - [x] Add credibility tracking for source platforms
- [x] Create batch processing system (AC: 6)
  - [x] Build `app/api/cron/synthesize/route.ts` for scheduled processing
  - [x] Implement efficient batching to stay within API rate limits
  - [x] Add queue management for pending content synthesis
  - [x] Create usage monitoring for OpenAI API consumption
- [x] Implement content categorization (AC: 7)
  - [x] Build automatic tagging system based on content topics
  - [x] Create category mapping aligned with user interests
  - [x] Add content organization for structured delivery
- [x] Build fallback and error handling (AC: 8)
  - [x] Implement API failure detection and retry logic
  - [x] Create content queuing system for failed synthesis attempts
  - [x] Add graceful degradation when OpenAI API is unavailable
  - [x] Build error logging and monitoring system
- [x] Unit testing implementation
  - [x] Write tests for OpenAI client and prompt engineering
  - [x] Test content synthesis pipeline with mock data
  - [x] Create tests for quality validation and scoring systems
  - [x] Add tests for batch processing and rate limiting
  - [x] Test error handling and fallback mechanisms

## Dev Notes

### Previous Story Insights
From Stories 2.1-2.3 completion:
- UserPreferences model fully operational with interests, tech_stack, delivery_time, and content_depth fields
- Content model established with source_urls, source_platform, source_metadata, relevance_score, and content_hash fields
- Multi-platform content sourcing system working (Reddit API, Product Hunt RSS)
- Rate limiting and usage monitoring systems operational for external APIs
- Content deduplication and relevance scoring systems functional

### Tech Stack Configuration
[Source: architecture/tech-stack.md#technology-stack-table]
- **Framework**: Next.js 14.2+ with API routes for cron job endpoints
- **Database**: PostgreSQL via Supabase with Prisma 5.0+ ORM for Content storage
- **AI Integration**: OpenAI GPT-5 mini API for content synthesis (default model)
- **API Client**: Native Fetch API for OpenAI integration
- **Deployment**: Vercel with Vercel Cron for scheduled synthesis jobs
- **Error Handling**: Console + Vercel built-in logging for API failures
- **Monitoring**: Vercel Analytics for basic API usage metrics

### Data Models
[Source: architecture/data-models.md]
**Existing Content Model (Extended from Story 2.3):**
```typescript
interface Content {
  id: string;
  user_id: string;
  title: string;
  summary: string;
  source_urls: string[];
  source_platform: string;      // From Story 2.3
  source_metadata: object;      // From Story 2.3
  relevance_score: number;      // From Story 2.3
  content_hash: string;         // From Story 2.3
  created_at: Date;
  delivered: boolean;
}
```

**Extensions Required for AI Synthesis:**
- Add `synthesis_metadata: object` for AI processing details
- Add `confidence_score: number` for synthesis quality tracking
- Add `synthesis_status: enum` ('pending', 'processing', 'completed', 'failed')
- Add `categories: string[]` for content categorization
- Add `processing_attempts: number` for retry tracking

**UserPreferences Fields for Personalization:**
```typescript
interface UserPreferences {
  interests: string[];          // For content relevance matching
  tech_stack: string[];         // For technical context injection
  delivery_time: string;        // For scheduling synthesis jobs
  content_depth: 'brief' | 'detailed'; // For synthesis depth control
}
```

### API Specifications
[Source: architecture/api-specification.md#admin/cron-endpoints]
**AI Synthesis Cron Endpoint:**
```typescript
POST /api/cron/synthesize    // Main AI content synthesis orchestrator
```

**Content Processing Workflow:**
1. Fetch unsynthesized content from Content table
2. Group content by user and relevance scores
3. Apply user context from UserPreferences
4. Generate AI synthesis with OpenAI API
5. Validate synthesis quality and confidence
6. Update Content records with synthesized insights
7. Mark content as ready for delivery

### External API Integration Patterns
**OpenAI GPT-5 Mini API Integration:**
- Use OpenAI GPT-5 mini model (`gpt-5-mini`) for cost-effective content synthesis
- **Pricing**: $0.25/1M input tokens, $2.00/1M output tokens (83% cost reduction vs GPT-4o)
- **Context Window**: 272,000 input tokens, 128,000 output tokens (including reasoning)
- **Knowledge Cutoff**: May 30, 2024
- **API Features**: Supports reasoning_effort (minimal/low/medium/high), verbosity (low/medium/high), structured outputs
- **Rate Limits**: Standard tier limits apply (check current OpenAI dashboard for account limits)
- **Usage tracking essential for budget management and cost optimization**

**GPT-5 Mini Prompt Engineering Strategy:**
- Utilize `reasoning_effort: "low"` for faster synthesis without extensive reasoning
- Set `verbosity: "medium"` for balanced response length appropriate for content synthesis
- Professional context injection from user preferences in system prompts
- Source attribution maintenance through structured outputs
- Content categorization using built-in reasoning capabilities
- Quality validation through confidence scoring in structured JSON responses

### File Locations and Project Structure
[Source: architecture/project-structure.md]
**AI Synthesis Implementation:**
- OpenAI Client: `lib/openai.ts` (create new)
- Synthesis Engine: `lib/content-synthesis.ts` (create new)
- Quality Validation: `lib/synthesis-quality.ts` (create new)
- Content Categorization: `lib/content-categorization.ts` (create new)

**API Routes:**
- Main Synthesis Job: `app/api/cron/synthesize/route.ts` (create new)
- Synthesis Status: `app/api/content/synthesis-status/route.ts` (create new)

**Utility Libraries:**
- OpenAI Client: `lib/openai.ts` (create new)
- Environment Config: `lib/env.ts` (already exists)
- Rate Limiting: `lib/rate-limit.ts` (already exists)

### Security and Cost Management
[Source: architecture/tech-stack.md#mvp-technology-decisions]
- Store OpenAI API key in environment variables securely
- **Critical: Monitor API usage to avoid unexpected costs**
- Implement conservative rate limiting below OpenAI maximums
- Use Vercel Cron protection to prevent unauthorized synthesis calls
- Add request timeout handling for OpenAI API calls
- Implement exponential backoff for failed API requests
- **Build cost dashboards to track daily/weekly OpenAI spending**

### Required Environment Variables
The following environment variables must be added to `.env.local`:

```bash
# OpenAI API Configuration (GPT-5 Mini)
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-5-mini
OPENAI_MAX_INPUT_TOKENS=272000
OPENAI_MAX_OUTPUT_TOKENS=4000  # Conservative limit for synthesis
OPENAI_TEMPERATURE=0.7
OPENAI_REASONING_EFFORT=low     # For faster processing
OPENAI_VERBOSITY=medium        # Balanced response length

# Synthesis Configuration
SYNTHESIS_BATCH_SIZE=10
SYNTHESIS_RATE_LIMIT=50  # Requests per minute (adjust based on account limits)
SYNTHESIS_RETRY_ATTEMPTS=3
SYNTHESIS_CONFIDENCE_THRESHOLD=0.7

# Cost monitoring settings (GPT-5 mini pricing)
OPENAI_COST_ALERT_THRESHOLD_USD=10.00
OPENAI_INPUT_COST_PER_1M_TOKENS=0.25
OPENAI_OUTPUT_COST_PER_1M_TOKENS=2.00
OPENAI_USAGE_LOG_LEVEL=info
```

### Testing Requirements
[Source: architecture/testing-strategy.md#testing-priorities-for-mvp]
**Critical Path Testing:**
- AI content synthesis pipeline from raw content to insights
- **Cost management and API usage monitoring**
- OpenAI API integration error handling
- Content quality validation and confidence scoring
- Synthesis personalization accuracy

**API Route Testing:**
- Synthesis cron job endpoint authentication and execution
- OpenAI API integration error scenarios
- Content processing and storage workflows
- Batch processing and rate limiting validation

### Coding Standards
[Source: architecture/coding-standards.md#critical-rules-for-ai-development]
- Use Prisma client for all Content model operations
- Type all OpenAI API responses with TypeScript interfaces
- Use environment variable validation with zod for API keys
- Implement consistent error handling across synthesis pipeline
- Follow API route patterns with proper JSON responses
- Use Server Components for content processing (no 'use client' needed)

### Testing

#### Test Requirements for This Story
[Source: architecture/testing-strategy.md#testing-priorities-for-mvp]
- Test complete AI synthesis pipeline from raw content to personalized insights
- Verify OpenAI API integration and error handling for service interruptions
- Test content quality validation and confidence scoring accuracy
- Validate personalization based on user preferences and context
- Test batch processing efficiency and rate limiting compliance
- Verify source attribution preservation through synthesis process
- Test cost monitoring and usage tracking systems

#### Test File Locations
- Unit tests: `/tests/unit/content-synthesis/`
- API tests: `/tests/api/ai-synthesis.test.ts`
- Integration tests: `/tests/integration/openai-integration.test.ts`

---

# Dev Agent Record

## Agent Model Used
- **Model**: claude-sonnet-4-20250514
- **Role**: James - Full Stack Developer
- **Date**: 2025-08-25

## Debug Log References
- Content synthesis implementation: lib/openai.ts, lib/content-synthesis.ts
- API endpoints: app/api/cron/synthesize/route.ts, app/api/content/synthesis-status/route.ts
- Testing: tests/unit/content-synthesis.test.ts
- QA Fix Applied: Changed OpenAI model from 'gpt-4o-mini' to 'gpt-5-mini' in lib/openai.ts:135
- Validation: Content synthesis tests passing (6/6)

## Completion Notes
- [x] Implemented complete OpenAI API integration with GPT-5 mini model (corrected from GPT-4o-mini)
- [x] Built content synthesis engine with user personalization 
- [x] Created batch processing system with rate limiting
- [x] Added comprehensive error handling and fallback mechanisms
- [x] Implemented source attribution and content categorization
- [x] Built monitoring endpoints for synthesis status
- [x] Created unit tests with proper mocking
- [x] Updated environment schema to require OPENAI_API_KEY

## File List
### New Files Created
- lib/openai.ts - OpenAI API client and prompt engineering
- lib/content-synthesis.ts - Main synthesis engine and processing pipeline
- app/api/cron/synthesize/route.ts - Scheduled batch synthesis endpoint
- app/api/content/synthesis-status/route.ts - Synthesis monitoring endpoint
- tests/unit/content-synthesis.test.ts - Unit tests for synthesis functionality

### Modified Files  
- lib/env.ts - Made OPENAI_API_KEY required for AI synthesis

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-25 | 1.0 | Initial story creation with full architecture context | Bob (Scrum Master) |
| 2025-08-25 | 1.1 | Updated to use GPT-5 mini with latest API conventions and pricing | Bob (Scrum Master) |
| 2025-08-25 | 2.0 | Implementation completed by James - Full Stack Developer | James (Dev Agent) |
| 2025-08-25 | 2.1 | QA Fix: Corrected OpenAI model from GPT-4o-mini to GPT-5 mini per story requirements | James (Dev Agent) |

## Status
Ready for Done

## QA Results

### Review Date: 2025-08-25

### Reviewed By: Quinn (Test Architect)

### Code Quality Assessment

The AI Content Synthesis Engine implementation demonstrates solid architectural design and comprehensive coverage of all acceptance criteria. The code follows Next.js 14 patterns well with proper TypeScript interfaces and error handling. The OpenAI integration is implemented efficiently using GPT-4o-mini with appropriate rate limiting and cost management. The synthesis pipeline properly handles user personalization and maintains source attribution throughout the process.

### Refactoring Performed

- **File**: lib/openai.ts
  - **Change**: Fixed TypeScript type safety in validateSynthesisResult function by adding proper type assertion
  - **Why**: Eliminated unsafe property access on unknown type that could cause runtime errors
  - **How**: Added explicit type assertion with proper interface definition for safer property access

### Compliance Check

- Coding Standards: ✓ Uses Prisma for database operations, proper TypeScript typing, follows API route patterns
- Project Structure: ✓ Files placed correctly in lib/, app/api/, and tests/ directories
- Testing Strategy: ✓ Comprehensive unit tests with proper mocking and validation
- All ACs Met: ✓ All 8 acceptance criteria fully implemented and tested

### Improvements Checklist

- [x] Fixed TypeScript type safety issue in OpenAI validation (lib/openai.ts)
- [x] Verified test coverage for all synthesis functionality
- [x] Confirmed proper error handling and fallback mechanisms
- [ ] Consider adding integration tests for end-to-end synthesis workflow
- [ ] Add cost monitoring alerts when approaching OpenAI API limits
- [ ] Consider implementing synthesis result caching for repeated requests

### Security Review

Security considerations are well addressed:
- OpenAI API key properly secured in environment variables with validation
- Cron endpoint protected with Bearer token authentication
- Rate limiting implemented to prevent API abuse
- Input validation for user preferences and content data
- No sensitive data logging in synthesis processes

### Performance Considerations

Performance is optimized for MVP requirements:
- GPT-4o-mini model selected for cost efficiency (83% reduction vs GPT-4o)
- Conservative rate limiting (500 requests/minute) to stay within API limits
- Batch processing with controlled delays between users
- Efficient database queries using Prisma with proper indexing
- Content limited to last 24 hours to manage processing scope

### Files Modified During Review

- lib/openai.ts - Fixed TypeScript type safety issue

### Gate Status

Gate: FAIL → docs/qa/gates/3.1-ai-content-synthesis-engine.yml
Critical Issue: Implementation uses GPT-4o-mini instead of required GPT-5 mini model
Risk profile: High-risk due to requirements deviation
NFR assessment: Technical requirements met but specification compliance failed

### Recommended Status

✗ Changes Required - Implementation deviates from story requirements by using GPT-4o-mini instead of GPT-5 mini model. Must fix model specification in lib/openai.ts:135 before deployment.